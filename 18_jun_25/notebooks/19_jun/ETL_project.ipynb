{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fefe29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Count: 99981\n"
     ]
    }
   ],
   "source": [
    "# PySpark ETL Case Study with Performance Tuning\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 1: SparkSession with tuned configurations\n",
    "# ---------------------------------------------\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SalesETL_PerformanceOptimized\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.shuffle.service.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# -------------------------\n",
    "# Step 2: Load CSV (Raw)\n",
    "# -------------------------\n",
    "# Simulate CSV source with a sample dataset\n",
    "data = [(i, \"Product\" + str(i % 5), \"Region\" + str(i % 3), i * 10.5) for i in range(1, 100001)]\n",
    "df_raw = spark.createDataFrame(data, [\"sale_id\", \"product\", \"region\", \"amount\"])\n",
    "\n",
    "# ------------------------------\n",
    "# Step 3: Column Pruning + Filter Pushdown\n",
    "# ------------------------------\n",
    "df_filtered = df_raw.filter(col(\"amount\") > 200).select(\"sale_id\", \"product\", \"amount\")\n",
    "\n",
    "# ------------------------------\n",
    "# Step 4: Repartition to increase parallelism\n",
    "# ------------------------------\n",
    "df_partitioned = df_filtered.repartition(8, \"product\")  # Shuffle but improves downstream parallelism\n",
    "\n",
    "# ------------------------------\n",
    "# Step 5: Cache intermediate result\n",
    "# ------------------------------\n",
    "df_cached = df_partitioned.cache()\n",
    "print(\"Filtered Count:\", df_cached.count())\n",
    "\n",
    "# -------------------------------------------\n",
    "# Step 6: Apply Aggregation with Vectorized UDF\n",
    "# -------------------------------------------\n",
    "@pandas_udf(\"double\")\n",
    "def discount_udf(amount: pd.Series) -> pd.Series:\n",
    "    return amount * 0.9  # Apply 10% discount\n",
    "\n",
    "# Apply transformation\n",
    "transformed_df = df_cached.withColumn(\"discounted_amount\", discount_udf(col(\"amount\")))\n",
    "\n",
    "# Aggregation per product\n",
    "agg_df = transformed_df.groupBy(\"product\").agg(\n",
    "    sum(\"discounted_amount\").alias(\"total_sales\"),\n",
    "    avg(\"discounted_amount\").alias(\"avg_sales\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a340a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "three layer >> raw files csv \n",
    ">> clean >> transfomation (4)\n",
    "# 3 files , import spark session , class object define ,import the from session file ,\n",
    "10g,\n",
    "2g,\n",
    "--- store file , analysis ,delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c4d3b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# ------------------------------\n",
    "# Step 7: Save as Partitioned Parquet (Columnar + Partitioning)\n",
    "# ------------------------------\n",
    "# agg_df.write.mode(\"overwrite\").partitionBy(\"product\").parquet(\"sales_agg_parquet\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38b80592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00756c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PySpark Performance Optimization: Concepts & Case Study\n",
    "\n",
    "# This document explains key PySpark performance optimization techniques followed by a unified ETL case study applying all of them.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ## ðŸ”¹ 1. Push-Down Filters / Predicate Pushdown\n",
    "\n",
    "# ### **What**: Filters are pushed to the data source to minimize data read.\n",
    "\n",
    "# ### **Why**: Reduces IO by only reading relevant rows.\n",
    "\n",
    "# ### **When**: Automatically enabled with Parquet, ORC, JDBC sources.\n",
    "\n",
    "# ### **If Not Used**: Entire data is loaded into Spark before filtering, leading to performance hits.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ## ðŸ”¹ 2. EXPLAIN and queryExecution\n",
    "\n",
    "# ### **What**: Tools to inspect Spark's logical and physical execution plans.\n",
    "\n",
    "# ### **Why**: Helps developers understand and tune query performance.\n",
    "\n",
    "# ### **When**: Use before heavy queries or when optimizing joins, filters.\n",
    "\n",
    "# ### **If Not Used**: You may end up with non-performant plans unknowingly.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ## ðŸ”¹ 3. repartition() vs coalesce()\n",
    "\n",
    "# ### **repartition()**: Increases partitions by full shuffle (expensive but good for parallelism).\n",
    "\n",
    "# ### **coalesce()**: Decreases partitions without full shuffle (use before writing to reduce small files).\n",
    "\n",
    "# ---\n",
    "\n",
    "# ## ðŸ”¹ 4. persist() and cache()\n",
    "\n",
    "# ### **What**: Store intermediate results in memory/disk for reuse.\n",
    "\n",
    "# ### **Why**: Avoid recomputation in iterative workloads.\n",
    "\n",
    "# ### **When**: Use when a DataFrame is reused multiple times.\n",
    "\n",
    "# ### **If Not Used**: Redundant computation wastes time and memory.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ## ðŸ”¹ 5. Columnar Storage Optimization\n",
    "\n",
    "# ### **What**: Use columnar formats like Parquet.\n",
    "\n",
    "# ### **Why**: Enables predicate pushdown and column pruning.\n",
    "\n",
    "# ### **When**: Always for disk-based storage.\n",
    "\n",
    "# ### **If Not Used**: Slower reads, higher IO.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ## ðŸ”¹ 6. Partitioning & Caching\n",
    "\n",
    "# ### **What**: Partitioning improves read parallelism, caching improves repeated reads.\n",
    "\n",
    "# ### **Why**: Better scan efficiency.\n",
    "\n",
    "# ### **When**: Large, often-read datasets (by columns like region, date).\n",
    "\n",
    "# ---\n",
    "\n",
    "# ## ðŸ”¹ 7. Shuffle Operations: Wide vs Narrow\n",
    "\n",
    "# * **Wide**: Causes shuffles (e.g., joins, groupBy).\n",
    "# * **Narrow**: No shuffle (e.g., map, filter).\n",
    "\n",
    "# ### **Multijoins**: Use broadcast joins where possible.\n",
    "\n",
    "# ### **Window Functions**: Avoid excessive shuffles, partition wisely.\n",
    "\n",
    "# ### **Shuffling Optimization**: Reduce data size and partitions.\n",
    "\n",
    "# ### **Partitioning Strategies**: Hash vs range, depends on data skew.\n",
    "\n",
    "# ### **CI/CD for Spark**: Use modular PySpark code, test with small data, lint and deploy.\n",
    "\n",
    "# ### **Configs**:\n",
    "\n",
    "# ```yaml\n",
    "# spark.executor.memory = 4g\n",
    "# spark.driver.memory = 2g\n",
    "# spark.default.parallelism = num_cores * 2\n",
    "# spark.sql.shuffle.partitions = 8\n",
    "# ```\n",
    "\n",
    "# ---\n",
    "\n",
    "# ## ðŸ”¹ 8. groupByKey() vs reduceByKey()\n",
    "\n",
    "# ### **groupByKey()**: Groups data, then applies logic. High shuffle cost.\n",
    "\n",
    "# ### **reduceByKey()**: Combines data during shuffle. More efficient for aggregations.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ## ðŸ”¹ 9. Configuring spark.sql.shuffle.partitions\n",
    "\n",
    "# ### **What**: Sets default number of partitions for shuffle operations.\n",
    "\n",
    "# ### **Why**: Controls parallelism & file output size.\n",
    "\n",
    "# ### **Best Practice**: Reduce from 200 (default) to optimal based on data size.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ## ðŸ”¹ 10. Avoid Python UDFs\n",
    "\n",
    "# ### **Why**: Slow, runs in Python interpreter outside JVM.\n",
    "\n",
    "# ### **Fix**: Use native Spark functions or Pandas UDF.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ## ðŸ”¹ 11. Pandas UDFs (Vectorized)\n",
    "\n",
    "# ### **Why**: Uses Arrow to process batches of data efficiently.\n",
    "\n",
    "# ### **When**: Custom logic needed with performance.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ## ðŸ”¹ 12. Arrow Optimization\n",
    "\n",
    "# ### **What**: Enables fast conversion between Spark and Pandas using Apache Arrow.\n",
    "\n",
    "# ### **When**: Use in `.toPandas()` or Pandas UDFs.\n",
    "\n",
    "# ---\n",
    "\n",
    "# # âœ… Case Study: Sales ETL Pipeline with Tuning\n",
    "\n",
    "# ### **Business Use Case**\n",
    "\n",
    "# Process and optimize a sales dataset for discounted product-level analytics.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### **Pipeline Steps**\n",
    "\n",
    "# 1. **Load**: Simulate or load sales data (CSV/Parquet).\n",
    "# 2. **Push-down Filter**: Filter rows where `amount > 200`.\n",
    "# 3. **Column Pruning**: Select only required columns.\n",
    "# 4. **Repartition**: Repartition by `product` before groupBy.\n",
    "# 5. **Cache**: Cache filtered data.\n",
    "# 6. **Pandas UDF**: Apply discount logic.\n",
    "# 7. **Aggregation**: `groupBy(product)` and aggregate.\n",
    "# 8. **Write**: Save results to Parquet partitioned by `product`.\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### **Configurations Used**\n",
    "\n",
    "# ```python\n",
    "# .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "# .config(\"spark.executor.memory\", \"2g\")\n",
    "# .config(\"spark.driver.memory\", \"2g\")\n",
    "# .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "# ```\n",
    "\n",
    "# ---\n",
    "\n",
    "# ### **Expected Outcome**\n",
    "\n",
    "# * Efficient shuffle operations.\n",
    "# * Minimal I/O.\n",
    "# * Fast memory reuse.\n",
    "# * Optimized write (partitioned Parquet).\n",
    "\n",
    "# ---\n",
    "\n",
    "# This unified pipeline reflects how PySpark performance best practices come together in real-world ETL jobs.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
