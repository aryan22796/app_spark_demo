{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0bceb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure of pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44661857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a1b898e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 , 20 core \n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"first_app_config\")\\\n",
    "        .master(\"local[*]\")\\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02ebd744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== config ==== printing\n",
      "('spark.driver.host', 'dell')\n",
      "('spark.app.submitTime', '1750219420834')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.driver.memory', '1g')\n",
      "('spark.driver.port', '52312')\n",
      "('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n",
      "('spark.sql.warehouse.dir', 'file:/E:/demo/notebooks/topic/spark-warehouse')\n",
      "('spark.app.id', 'local-1750219422266')\n",
      "('spark.executor.memory', '1g')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.master', 'local[*]')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.app.startTime', '1750219421038')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.ui.showConsoleProgress', 'true')\n",
      "('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n",
      "('spark.app.name', 'first_app_config')\n"
     ]
    }
   ],
   "source": [
    "print(\"======== config ==== printing\")\n",
    "for items in spark.sparkContext.getConf().getAll():\n",
    "    print(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84e72011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Setting Dynamic Configuration ===\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n=== Setting Dynamic Configuration ===\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"10\")   # Set number of partitions for shuffle\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500f2c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuff_part = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "arrow_enable = spark.conf.get(\"spark.sql.execution.arrow.pyspark.enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41dd6eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(shuff_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8e6129f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    }
   ],
   "source": [
    "print(arrow_enable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20cf6370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non config\n",
      "caught Error An error occurred while calling o143.get.\n",
      ": java.util.NoSuchElementException: non.existing.config\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.noSuchElementExceptionError(QueryExecutionErrors.scala:2138)\n",
      "\tat org.apache.spark.sql.internal.SQLConf.$anonfun$getConfString$3(SQLConf.scala:5041)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.internal.SQLConf.getConfString(SQLConf.scala:5041)\n",
      "\tat org.apache.spark.sql.RuntimeConfig.get(RuntimeConfig.scala:81)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"non config\")\n",
    "    print(spark.conf.get(\"non.existing.config\"))\n",
    "    #print(spark.conf.get(\"spark.sql.shuffle.partitions\"))\n",
    "except Exception as e:\n",
    "    print(\"caught Error\", e)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d42441a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start >> session >> conf.set >>> for reading conf you get(conf.get)>> getAll()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f40964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#key , value\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"driver_executor\")\\\n",
    "    .config(\"spark.driver.memory\",\"1g\")\\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\")\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d79b25c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1g\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(spark.conf.get(\"spark.driver.memory\"))\n",
    "print(spark.conf.get(\"spark.executor.cores\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db852215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(spark.conf.get(\"spark.driver.cores\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7a3756",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
