{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc75fe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark Configuration Basics and Dynamic Config Example\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession with initial configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Configuration Demo\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef26e5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Spark Configurations (Initial) ===\n",
      "('spark.app.name', 'PySpark Configuration Demo')\n",
      "('spark.driver.host', 'dell')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.driver.memory', '1g')\n",
      "('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n",
      "('spark.app.id', 'local-1750190885624')\n",
      "('spark.executor.memory', '1g')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark.driver.port', '51652')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.master', 'local[*]')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.app.startTime', '1750190885569')\n",
      "('spark.app.submitTime', '1750190853074')\n",
      "('spark.ui.showConsoleProgress', 'true')\n",
      "('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n",
      "\n",
      "=== Setting Dynamic Configuration ===\n",
      "Shuffle Partitions: 10\n",
      "Arrow Enabled: true\n"
     ]
    }
   ],
   "source": [
    "# Show all Spark configurations (key-value pairs)\n",
    "print(\"=== Spark Configurations (Initial) ===\")\n",
    "for item in spark.sparkContext.getConf().getAll():\n",
    "    print(item)\n",
    "\n",
    "# === Dynamically set configuration at runtime ===\n",
    "print(\"\\n=== Setting Dynamic Configuration ===\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"10\")   # Set number of partitions for shuffle\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")  # Enable Arrow optimization\n",
    "\n",
    "# Read back the values\n",
    "shuffle_partitions = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "arrow_enabled = spark.conf.get(\"spark.sql.execution.arrow.pyspark.enabled\")\n",
    "\n",
    "print(f\"Shuffle Partitions: {shuffle_partitions}\")\n",
    "print(f\"Arrow Enabled: {arrow_enabled}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4319f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Trying to fetch non-existent config ===\n",
      "Caught Error: An error occurred while calling o170.get.\n",
      ": java.util.NoSuchElementException: non.existing.config\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.noSuchElementExceptionError(QueryExecutionErrors.scala:2138)\n",
      "\tat org.apache.spark.sql.internal.SQLConf.$anonfun$getConfString$3(SQLConf.scala:5041)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.internal.SQLConf.getConfString(SQLConf.scala:5041)\n",
      "\tat org.apache.spark.sql.RuntimeConfig.get(RuntimeConfig.scala:81)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Invalid Configuration Test (Try / Except) ===\n",
    "try:\n",
    "    print(\"\\n=== Trying to fetch non-existent config ===\")\n",
    "    print(spark.conf.get(\"non.existing.config\"))\n",
    "except Exception as e:\n",
    "    print(\"Caught Error:\", e)\n",
    "\n",
    "# Stop session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "581dfe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | Section                         | Purpose                        |\n",
    "# | ------------------------------- | ------------------------------ |\n",
    "# | `SparkSession.builder.config()` | Static config at session start |\n",
    "# | `spark.conf.set()`              | Dynamic config at runtime      |\n",
    "# | `spark.conf.get()`              | Read configs dynamically       |\n",
    "# | `getAll()`                      | List all configs               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c5373ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è 1. Why Configuration Matters in PySpark?\n",
    "# Spark has hundreds of configuration options. You can:\n",
    "\n",
    "# Allocate memory and CPU resources.\n",
    "\n",
    "# Control parallelism (e.g., number of partitions).\n",
    "\n",
    "# Tune performance (e.g., caching, shuffle behavior).\n",
    "\n",
    "# Enable/disable features (e.g., Arrow, adaptive execution).\n",
    "\n",
    "# You can set configs in 2 ways:\n",
    "\n",
    "# Static: Before creating SparkSession.\n",
    "\n",
    "# Dynamic: After session starts using spark.conf.set().\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f56491e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ConfigDemo\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ff9e571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# Set configuration\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"10\")\n",
    "\n",
    "# Get configuration\n",
    "print(spark.conf.get(\"spark.sql.shuffle.partitions\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c554684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Static Spark Configurations ===\n",
      "spark.app.name: PySpark Config Demo\n",
      "spark.driver.host: dell\n",
      "spark.executor.id: driver\n",
      "spark.driver.memory: 1g\n",
      "spark.driver.extraJavaOptions: -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.executor.memory: 1g\n",
      "spark.rdd.compress: True\n",
      "spark.app.startTime: 1750191070042\n",
      "spark.serializer.objectStreamReset: 100\n",
      "spark.driver.port: 51876\n",
      "spark.app.id: local-1750191070102\n",
      "spark.master: local[*]\n",
      "spark.submit.pyFiles: \n",
      "spark.submit.deployMode: client\n",
      "spark.app.submitTime: 1750190853074\n",
      "spark.ui.showConsoleProgress: true\n",
      "spark.executor.extraJavaOptions: -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "\n",
      "=== Dynamic Configs After Setting ===\n",
      "spark.sql.shuffle.partitions: 10\n",
      "spark.sql.execution.arrow.pyspark.enabled: true\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Step 1: Create SparkSession with static configs\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Config Demo\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Print all static configs\n",
    "print(\"=== Static Spark Configurations ===\")\n",
    "for k, v in spark.sparkContext.getConf().getAll():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# Step 3: Set dynamic configs\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"10\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "# Step 4: Read and print dynamic configs\n",
    "print(\"\\n=== Dynamic Configs After Setting ===\")\n",
    "print(\"spark.sql.shuffle.partitions:\", spark.conf.get(\"spark.sql.shuffle.partitions\"))\n",
    "print(\"spark.sql.execution.arrow.pyspark.enabled:\", spark.conf.get(\"spark.sql.execution.arrow.pyspark.enabled\"))\n",
    "\n",
    "# Step 5: Try reading a non-existing config key (to show error handling)\n",
    "# try:\n",
    "#     print(\"\\nTrying to get non.existing.config:\")\n",
    "#     print(spark.conf.get(\"non.existing.config\"))\n",
    "# except Exception as e:\n",
    "#     print(\"Caught error:\", e)\n",
    "\n",
    "# Step 6: Stop session (optional at end of notebook)\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76dcc305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | Feature                 | Static Config           | Dynamic Config                 |\n",
    "# | ----------------------- | ----------------------- | ------------------------------ |\n",
    "# | Set Before SparkSession | ‚úÖ                       | ‚ùå                              |\n",
    "# | Set After SparkSession  | ‚ùå                       | ‚úÖ                              |\n",
    "# | Examples                | `spark.executor.memory` | `spark.sql.shuffle.partitions` |\n",
    "# | Requires Restart        | ‚úÖ                       | ‚ùå                              |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f671324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | Scenario                              | Configuration                                      |\n",
    "# | ------------------------------------- | -------------------------------------------------- |\n",
    "# | Want to reduce shuffle overhead       | `spark.sql.shuffle.partitions = 10`                |\n",
    "# | Want to enable Arrow with Pandas UDFs | `spark.sql.execution.arrow.pyspark.enabled = true` |\n",
    "# | For big joins                         | `spark.sql.autoBroadcastJoinThreshold`             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee76a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .conf.set(\"spark.sql.shuffle.partitions\", \"10\")\n",
    "# üìå What it does:\n",
    "# This sets the number of partitions to use when shuffling data in Spark SQL operations (like groupBy, join, orderBy, etc.).\n",
    "\n",
    "# üí° What is \"shuffle\"?\n",
    "# Shuffling = Moving data between partitions (e.g., different worker nodes) during distributed operations. It's expensive (in terms of time and resources).\n",
    "\n",
    "# üß† Why change this?\n",
    "# Default is 200 partitions ‚Äî too many for small datasets.\n",
    "\n",
    "# If you're running on a laptop or small cluster, set this to 10 or lower to reduce overhead.\n",
    "\n",
    "# ‚úÖ Summary:\n",
    "# Tells Spark: \"Use 10 partitions instead of 200 during shuffle-heavy operations (like groupBy, join).\"\n",
    "\n",
    "# ‚úÖ 2. spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "# üìå What it does:\n",
    "# This enables Apache Arrow for faster data transfer between Spark and Pandas (especially for .toPandas() and Pandas UDFs).\n",
    "\n",
    "# üí° What is Arrow?\n",
    "# Arrow is a high-performance, in-memory format that makes Spark ‚Üî Pandas communication much faster.\n",
    "\n",
    "# üß† Why use it?\n",
    "# If you often do:\n",
    "\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# df.toPandas()\n",
    "# or\n",
    "\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# @pandas_udf(...)\n",
    "# def my_udf(...):\n",
    "#     ...\n",
    "# Then this config can boost performance 10x+.\n",
    "\n",
    "# ‚úÖ Summary:\n",
    "# Tells Spark: \"Use Apache Arrow to speed up communication between Spark and Pandas (for DataFrame conversion and Pandas UDFs).\"\n",
    "\n",
    "# üéØ Real Use Case Example\n",
    "# python\n",
    "# Copy\n",
    "# Edit\n",
    "# # This will be faster when Arrow is enabled\n",
    "# pdf = spark.read.csv(\"data.csv\", header=True).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1586401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
